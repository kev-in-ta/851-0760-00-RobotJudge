1. Overview

A common refrain is that “a picture is worth a thousand words.” In the paper, “Understanding the Political Ideology of Legislators from Social Media Images” by Xi et al., the images posted by United States Members of Congress on Facebook are run through several image analysis and deep learning models to identify ideological and political visual signatures between Republicans and Democrats. Images are often used to elicit a strong emotional and instinctual reaction and have become a powerful tool for partisan rhetoric in the United States. The paper explicitly lists three questions it hopes to answer:

1. Can machine learning identify party affiliation and political ideology of politicians from their Facebook photographs?
2. Can humans identify the party affiliation as well as a deep learning classifier? Do humans achieve a better accuracy or attend to the same visual features which the classifier utilizes?
3. Which visual features are associated with liberals or conservatives?

I would argue that the authors presented their work clearly and with well-justified controls for confounders within their models. They presented reasonable interpretation of the results with past literature to support their hypotheses. My major recommendation would be to explore some of the factors they controlled for, such as the removal of women and people-of-colour, as these elements are crucial divides in the discourse of American political demographics. I have some concern over the removal of these groups as it potentially contributes towards academic marginalization of minorities and their use of visual rhetoric is deserving of exploration in future work.

2. Review of Methods & Results

The first interesting step the authors used to build and control for confounders involved data selection. After scraping the images, they were able to gather a dataset of 296,461 images for 319 Members of Congress (out of 535). Within this set, two winnowing processes are performed. Due to the high correlation between women (66 members) and people-of-colour (55 members) to the Democratic Party, they were removed from the list leaving a demographic of white males. Next, politicians with less than 150 images were also removed to insure adequate training data. A balanced dataset was constructed from the remaining samples with 68 representatives from each party and 150 photos from each representative. This controls feel well-motivated in preventing over-fitting within the models due to small samples sizes or specifically recognizing gender or racial features.

The main classification of ideology utilizes the NOMINATE framework developed by Poole and Rosenthal in 2000 and which remains the “gold standard” for ideological scoring across the left-right continuum. This algorithm uses past voting records to score politicians on the ideological continuum, where conservatism is related to resistance to social change and accepting existing inequality, while liberalism is associated with embracing social change and a stronger preference for redistributive policies. 

The analysis utilized one NN to classify along party affiliations using the entire holistic image, three different CNNs models to analyze facial features, and one object-based classification to identify objects associated with certain ideologies. 

The first neural network achieves 59.28% accuracy off a single image and 82.35% accuracy when utilizing the entire set of a politician’s images. Considering the fact that political visual rhetoric may bear significant similarities / overlap between parties, the single image accuracy is promising even though it’s only a 9.28% improvement over random chance. With an image ensemble, the results make an even stronger case of a shared visual language within each of the two different parties. This information is summarized in a graph plotting the model output against NOMINATE scores which show a clear correlation between the two outputs. 

The facial recognition CNNs aimed to produce information about the expression, gender, and race of the main politician and the associates in each photograph. The expressions were classified as angry, disgusted, afraid, happy, sad, surprised, or neutral. The authors found that the expression (%) difference was statistically significant. However, I found this particular result less motivated than the other metrics used. Their past literature seems to have some conflicting results with some sources saying conservatives tend to be happier (Carroll, 2007, Taylor, Funk, and Craighill 2006) while another source saying the liberals tend to be happier (Wojcik et al., 2015). Facial features in expression have some degree of subjectivity and I think I would need a larger exploration within this field to be convinced. The other metrics, which is the prevalence of gender and racial make-up of the associates (as the main politicians were controlled to be white and male) was found to be statistically significant, which has already been well motivated in the previous discussion. This information was primarily compiled into tables which were quite each to decipher and have their associated p-value for identifying significance.

Finally, they utilized the Google Vision API to identify specific objects within photographs and then looked at their relative frequency for each party. I think the results they presented reflect what you might expect with the features associated with Republicans (“military officer,” “military person,” “award,” “senior citizen,” “court”) all point towards nationalistic and historical concepts consistent with right-leaning ideologies, whereas features associated with Democrats (“sea,” “ocean,” “wilderness,” “metropolitan,” “education”) point more towards issues like addressing climate change or inequality. The highest correlated results were grouped together in a table with their associate p-value and was also easy to decipher.

Finally, their last analysis focused on comparing between human classification of images and how it compares to the model on 272 randomly selected and balanced image set. Here they found similar accuracy between two (61.0% vs. 63.6%) with the model slightly outperforming but not in a statistically significant amount. Humans were found to be better at discerning features which may not be presented in high enough volume in the training sets for the NN to identify (e.g. red Habitats for Humanity shirts identified as Democrat leaning by humans), whereas the model identified certain subtle features, such as the appearance of the Capitol Building, correlating more strongly with the Republican Party. This information is summarized in a confusion matrix which is a great way to showcase these binary classification results. 

One aspect of note, however, is that humans classified more of the dataset as Republican, which the authors state might be due to the racial and gender make-up of the politicians. This feature, again leads back to my recommendation in the outline where I believe the stark racial and gender divide in American political discourse is so pivotal that controlling for it has some unintended consequences. While I won’t argue that the decision is wrong due to the concerns of over fitting, I think the visual language of traditionally marginalized groups deserve addition analysis and presents an area of significant weakness to me as a reader. 

3. Conclusion

This is an effective paper which constructs a reasonable dataset and makes reasonable choices in what they control for. They set out to answer three specific questions and they found that:

1. Machine learning is capable of identifying ideological trends within the media shared by politicians.
2. Humans and deep learning models can exhibit similar accuracy in identifying political leanings within images, with some caveats to the balancing of the data.
3. There are features which, with statistical significance, are correlated to a specific party affiliation.

While I do have concerns over the elimination of women and people of colour from the datasets, I don’t believe it was the wrong choice. A further examination of the visual language of women and people of colour in politics is definitely needed in future works of this kind. 

This work presents contributions towards understanding another tool in the rhetoric used by politicians and gives insight into how visual signatures and language can elicit reactions from people exposed to such media. Understanding these elements can help people make informed decisions and identify hidden context when images are shared. 